{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Implementation of tokenization methods without using the Libraries for Engish and Hausa Language"
      ],
      "metadata": {
        "id": "VtDvrvaOGLTo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_KNTcx2qDH79",
        "outputId": "b137290a-29aa-41f4-dab7-aea54014d66b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Tokenization Program!\n",
            "Enter a sentence in English: welcome to my home !\n",
            "Enter a sentence in Hausa: barka da zuwa gida na\n",
            "Enter a custom delimiter for tokenization: n\n",
            "Enter a regex pattern for tokenization (e.g., '[.!?]'): !\n",
            "\n",
            "Tokenization Results:\n",
            "\n",
            "--- English Sentence ---\n",
            "Original Sentence: welcome to my home !\n",
            "1. Whitespace Tokenization: ['welcome', 'to', 'my', 'home', '!']\n",
            "2. Punctuation-Based Tokenization: ['welcome to my home ', '!']\n",
            "3. Character Tokenization: ['w', 'e', 'l', 'c', 'o', 'm', 'e', ' ', 't', 'o', ' ', 'm', 'y', ' ', 'h', 'o', 'm', 'e', ' ', '!']\n",
            "4. Custom Delimiter Tokenization (Delimiter='n'): ['welcome to my home !']\n",
            "5. Regex-Based Tokenization (Pattern='!'): ['welcome to my home ', '']\n",
            "\n",
            "--- Hausa Sentence ---\n",
            "Original Sentence: barka da zuwa gida na\n",
            "1. Whitespace Tokenization: ['barka', 'da', 'zuwa', 'gida', 'na']\n",
            "2. Punctuation-Based Tokenization: ['barka da zuwa gida na']\n",
            "3. Character Tokenization: ['b', 'a', 'r', 'k', 'a', ' ', 'd', 'a', ' ', 'z', 'u', 'w', 'a', ' ', 'g', 'i', 'd', 'a', ' ', 'n', 'a']\n",
            "4. Custom Delimiter Tokenization (Delimiter='n'): ['barka da zuwa gida ', 'a']\n",
            "5. Regex-Based Tokenization (Pattern='!'): ['barka da zuwa gida na']\n",
            "\n",
            "--- Byte Pair Encoding (BPE) ---\n",
            "English Sentence BPE: ['w el c o m e </w>', 't o </w>', 'm y </w>', 'h o m e </w>', '! </w>']\n",
            "Hausa Sentence BPE: ['b a r k a </w>', 'd a </w>', 'z u wa </w>', 'g i d a </w>', 'n a </w>']\n"
          ]
        }
      ],
      "source": [
        "def whitespace_tokenization(sentence):\n",
        "    return sentence.split()\n",
        "\n",
        "def punctuation_based_tokenization(sentence):\n",
        "    punctuations = '.,!?;:()[]{}\"\\''\n",
        "    tokens = []\n",
        "    word = ''\n",
        "    for char in sentence:\n",
        "        if char in punctuations:\n",
        "            if word:\n",
        "                tokens.append(word)\n",
        "                word = ''\n",
        "            tokens.append(char)\n",
        "        else:\n",
        "            word += char\n",
        "    if word:\n",
        "        tokens.append(word)\n",
        "    return tokens\n",
        "\n",
        "def character_tokenization(sentence):\n",
        "    return list(sentence)\n",
        "\n",
        "def custom_delimiter_tokenization(sentence, delimiter):\n",
        "    return sentence.split(delimiter)\n",
        "\n",
        "def regex_based_tokenization(sentence, pattern):\n",
        "    import re\n",
        "    return re.split(pattern, sentence)\n",
        "\n",
        "def byte_pair_encoding(sentence, merge_operations):\n",
        "    vocab = {}\n",
        "    for word in sentence.split():\n",
        "        word = ' '.join(list(word)) + ' </w>'\n",
        "        vocab[word] = vocab.get(word, 0) + 1\n",
        "\n",
        "    for merge_op in merge_operations:\n",
        "        new_vocab = {}\n",
        "        for word in vocab:\n",
        "            new_word = word.replace(' '.join(merge_op), ''.join(merge_op))\n",
        "            new_vocab[new_word] = vocab[word]\n",
        "        vocab = new_vocab\n",
        "\n",
        "    return list(vocab.keys())\n",
        "\n",
        "# User Input\n",
        "print(\"Welcome to the Tokenization Program!\")\n",
        "sentence_en = input(\"Enter a sentence in English: \")\n",
        "sentence_ha = input(\"Enter a sentence in Hausa: \")\n",
        "delimiter = input(\"Enter a custom delimiter for tokenization: \")\n",
        "regex_pattern = input(\"Enter a regex pattern for tokenization (e.g., '[.!?]'): \")\n",
        "\n",
        "# Tokenization Outputs\n",
        "print(\"\\nTokenization Results:\")\n",
        "print(\"\\n--- English Sentence ---\")\n",
        "print(\"Original Sentence:\", sentence_en)\n",
        "print(\"1. Whitespace Tokenization:\", whitespace_tokenization(sentence_en))\n",
        "print(\"2. Punctuation-Based Tokenization:\", punctuation_based_tokenization(sentence_en))\n",
        "print(\"3. Character Tokenization:\", character_tokenization(sentence_en))\n",
        "print(\"4. Custom Delimiter Tokenization (Delimiter='{}'):\".format(delimiter), custom_delimiter_tokenization(sentence_en, delimiter))\n",
        "print(\"5. Regex-Based Tokenization (Pattern='{}'):\".format(regex_pattern), regex_based_tokenization(sentence_en, regex_pattern))\n",
        "\n",
        "print(\"\\n--- Hausa Sentence ---\")\n",
        "print(\"Original Sentence:\", sentence_ha)\n",
        "print(\"1. Whitespace Tokenization:\", whitespace_tokenization(sentence_ha))\n",
        "print(\"2. Punctuation-Based Tokenization:\", punctuation_based_tokenization(sentence_ha))\n",
        "print(\"3. Character Tokenization:\", character_tokenization(sentence_ha))\n",
        "print(\"4. Custom Delimiter Tokenization (Delimiter='{}'):\".format(delimiter), custom_delimiter_tokenization(sentence_ha, delimiter))\n",
        "print(\"5. Regex-Based Tokenization (Pattern='{}'):\".format(regex_pattern), regex_based_tokenization(sentence_ha, regex_pattern))\n",
        "\n",
        "# Byte Pair Encoding Example\n",
        "merge_ops_en = [('l', 'o'), ('H', 'e'), ('e', 'l'), ('l', 'o')]\n",
        "merge_ops_ha = [('g', 'w'), ('w', 'a'), ('a', 'j'), ('j', 'i')]\n",
        "print(\"\\n--- Byte Pair Encoding (BPE) ---\")\n",
        "print(\"English Sentence BPE:\", byte_pair_encoding(sentence_en, merge_ops_en))\n",
        "print(\"Hausa Sentence BPE:\", byte_pair_encoding(sentence_ha, merge_ops_ha))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Implementation of Stemming  methods without using the Libraries for Engish and Hausa Language"
      ],
      "metadata": {
        "id": "rn8j04AJGkWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming Methods\n",
        "def porter_stemmer(word):\n",
        "    # Simplified Porter Stemmer\n",
        "    if word.endswith(\"ing\") or word.endswith(\"ed\") or word.endswith(\"es\"):\n",
        "        return word[:-3]\n",
        "    elif word.endswith(\"s\") and len(word) > 1:\n",
        "        return word[:-1]\n",
        "    return word\n",
        "\n",
        "def snowball_stemmer(word):\n",
        "    # Snowball-like Stemmer (Handle \"ing\", \"ed\", \"es\")\n",
        "    if word.endswith(\"ing\") or word.endswith(\"ed\") or word.endswith(\"es\"):\n",
        "        return word[:-3]\n",
        "    elif word.endswith(\"ly\"):\n",
        "        return word[:-2]\n",
        "    return word\n",
        "\n",
        "def suffix_stripping_stemmer(word):\n",
        "    # Suffix stripping based on simple rules\n",
        "    suffixes = [\"ing\", \"ed\", \"es\", \"ly\", \"s\"]\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)]\n",
        "    return word\n",
        "\n",
        "def custom_rule_based_stemmer(word):\n",
        "    # Custom rules for stemming\n",
        "    if word.endswith(\"ies\"):\n",
        "        return word[:-3] + \"y\"\n",
        "    elif word.endswith(\"ing\"):\n",
        "        return word[:-3]\n",
        "    elif word.endswith(\"s\") and len(word) > 1:\n",
        "        return word[:-1]\n",
        "    return word\n",
        "\n",
        "# Lemmatization Methods\n",
        "def rule_based_lemmatization(word):\n",
        "    # Rule-based Lemmatization\n",
        "    if word in [\"running\", \"ran\", \"runs\"]:\n",
        "        return \"run\"\n",
        "    elif word in [\"jumps\", \"jumped\", \"jumping\"]:\n",
        "        return \"jump\"\n",
        "    return word\n",
        "\n",
        "def dictionary_based_lemmatization(word):\n",
        "    # Dictionary-based Lemmatization\n",
        "    lemma_dict = {\n",
        "        \"running\": \"run\",\n",
        "        \"ran\": \"run\",\n",
        "        \"jumps\": \"jump\",\n",
        "        \"jumped\": \"jump\",\n",
        "        \"children\": \"child\",\n",
        "        \"better\": \"good\",\n",
        "        \"worst\": \"bad\"\n",
        "    }\n",
        "    return lemma_dict.get(word, word)\n",
        "\n",
        "# Process input sentences\n",
        "def process_sentence(sentence, stemming_function, lemmatization_function):\n",
        "    words = sentence.split()\n",
        "    stemmed_words = [stemming_function(word) for word in words]\n",
        "    lemmatized_words = [lemmatization_function(word) for word in words]\n",
        "    return stemmed_words, lemmatized_words\n",
        "\n",
        "# User Input\n",
        "print(\"Welcome to the Stemming and Lemmatization Program!\")\n",
        "sentence_en = input(\"Enter a sentence in English: \")\n",
        "sentence_ha = input(\"Enter a sentence in Hausa: (Stemming only, lemmatization not supported): \")\n",
        "\n",
        "# English Processing\n",
        "print(\"\\n--- English Stemming and Lemmatization ---\")\n",
        "print(\"Original Sentence:\", sentence_en)\n",
        "\n",
        "# Apply Stemming\n",
        "for name, stemmer in [\n",
        "    (\"Porter Stemmer\", porter_stemmer),\n",
        "    (\"Snowball Stemmer\", snowball_stemmer),\n",
        "    (\"Suffix Stripping Stemmer\", suffix_stripping_stemmer),\n",
        "    (\"Custom Rule-Based Stemmer\", custom_rule_based_stemmer)\n",
        "]:\n",
        "    stemmed, lemmatized = process_sentence(sentence_en, stemmer, rule_based_lemmatization)\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(\"Stemmed Words:\", stemmed)\n",
        "\n",
        "# Apply Lemmatization\n",
        "print(\"\\n--- Lemmatization ---\")\n",
        "lemmatized_rule = [rule_based_lemmatization(word) for word in sentence_en.split()]\n",
        "lemmatized_dict = [dictionary_based_lemmatization(word) for word in sentence_en.split()]\n",
        "print(\"Rule-Based Lemmatization:\", lemmatized_rule)\n",
        "print(\"Dictionary-Based Lemmatization:\", lemmatized_dict)\n",
        "\n",
        "# Hausa Processing (Stemming Only)\n",
        "print(\"\\n--- Hausa Stemming ---\")\n",
        "print(\"Original Sentence:\", sentence_ha)\n",
        "stemmed_hausa = [suffix_stripping_stemmer(word) for word in sentence_ha.split()]\n",
        "print(\"Stemmed Words:\", stemmed_hausa)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ptuXUn0zGzQ0",
        "outputId": "14dd9690-7a79-494b-d1a1-71c4e23acb86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Stemming and Lemmatization Program!\n",
            "Enter a sentence in English: welcome to my world\n",
            "Enter a sentence in Hausa: (Stemming only, lemmatization not supported): barka da zuwa duniya ta\n",
            "\n",
            "--- English Stemming and Lemmatization ---\n",
            "Original Sentence: welcome to my world\n",
            "\n",
            "Porter Stemmer:\n",
            "Stemmed Words: ['welcome', 'to', 'my', 'world']\n",
            "\n",
            "Snowball Stemmer:\n",
            "Stemmed Words: ['welcome', 'to', 'my', 'world']\n",
            "\n",
            "Suffix Stripping Stemmer:\n",
            "Stemmed Words: ['welcome', 'to', 'my', 'world']\n",
            "\n",
            "Custom Rule-Based Stemmer:\n",
            "Stemmed Words: ['welcome', 'to', 'my', 'world']\n",
            "\n",
            "--- Lemmatization ---\n",
            "Rule-Based Lemmatization: ['welcome', 'to', 'my', 'world']\n",
            "Dictionary-Based Lemmatization: ['welcome', 'to', 'my', 'world']\n",
            "\n",
            "--- Hausa Stemming ---\n",
            "Original Sentence: barka da zuwa duniya ta\n",
            "Stemmed Words: ['barka', 'da', 'zuwa', 'duniya', 'ta']\n"
          ]
        }
      ]
    }
  ]
}